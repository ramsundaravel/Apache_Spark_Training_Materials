Details - Duration 15 to 20 minutes
Data is available in HDFS file system under /public/crime/csv
Structure of data (ID,0
Case Number,1
Date,2
Block,3
IUCR,4
Primary Type,5
Description,6
Location Description,7
Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location)
File format - text file
Delimiter - “,” (use regex while splitting split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1), as there are some fields with comma and enclosed using double quotes.
Get top 3 crime types based on number of incidents in RESIDENCE area using “Location Description”
Store the result in HDFS path /user/<YOUR_USER_ID>/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA
Output Fields: Crime Type, Number of Incidents
Output File Format: JSON
Output Delimiter: N/A
Output Compression: No

pyspark --master yarn

raw = sc.textFile("/public/crime/csv")
header = raw.first()
rawnohead = raw.filter(lambda x : x <> header)

ft = rawnohead.map(lambda x : x.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1))\
			  .map(lambda x : x[0].split(","))\
			  .map(lambda x : (x[5],x[7]))\
			  .filter(lambda x : x[1].upper()=='RESIDENCE')\
			  .map(lambda x : (x[0],1))\
			  .reduceByKey(lambda x ,y : x+y)\
			  .toDF(["crime type","Number of Incidents"])\
			  .orderBy(["Number of Incidents"],ascending = [0])\
			  .limit(3)

	
for i in ft.take(100) : print(i)

ft.coalesce(1).write.json("/user/ramkarthik/ram/solution/crime/RESIDENCE_AREA_CRIMINAL_TYPE_DATA.json",mode = "overwrite")


# Answer
# Row(crime type=u'BATTERY', Number of Incidents=244394)
# Row(crime type=u'OTHER OFFENSE', Number of Incidents=184667)
# Row(crime type=u'THEFT', Number of Incidents=142273)