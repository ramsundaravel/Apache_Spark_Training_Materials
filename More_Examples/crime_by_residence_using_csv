# Problem statement
# Data is available in HDFS file system under /public/crime/csv
# Structure of data (ID,Case Number,Date,IUCR,Primary Type,
# Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X #Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location)
# File format - text file
# Delimiter - “,” (use regex while splitting split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1), as there #  are some fields with comma and enclosed using double quotes.

# Get top 3 crime types based on number of incidents in RESIDENCE area using “Location Description”
# Store the result in HDFS path /user/<YOUR_USER_ID>/solutions/solution03/ 
# RESIDENCE_AREA_CRIMINAL_TYPE_DATA
# Output Fields: Crime Type, Number of Incidents
# Output File Format: text format
# Output Delimiter: N/A
# Output Compression: No


pyspark --master yarn --num-executors 8 --executor-memory 1G --packages com.databricks:spark-csv_2.10:1.5.0
sqlContext.setConf("spark.sql.shuffle.partitions","2")
from pyspark.sql.functions import *

raw = sqlContext.read.format("com.databricks.spark.csv").options(header='true',inferschema='true').load("/public/crime/csv")

ft0 = raw.select([col("Location Description").alias("Description"),col("Primary Type").alias("crime_type")])

ft0.registerTempTable("crime")

result = sqlContext.sql("select crime_type , count(1) as cnt from crime where upper(Description)='RESIDENCE' group by crime_type order by cnt desc")

result_write = result.select([col("crime_type").alias("Crime Type"),col("cnt").alias("Number of Incidents")])


result_write.limit(3).coalesce(1).write.format("com.databricks.spark.csv").mode("overwrite").options(header='true',codec=None).save("/user/ramkarthik/ram/solution/crime/RESIDENCE_AREA_CRIMINAL_TYPE_DATA987.csv")