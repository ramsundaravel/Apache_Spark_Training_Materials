Details - Duration 40 minutes
Data set URL90
Choose language of your choice Python or Scala
Data is available in HDFS file system under /public/crime/csv
You can check properties of files using hadoop fs -ls -h /public/crime/csv
Structure of data (ID,0
Case Number,1
Date,2
Block,3
IUCR,4
Primary Type,5
Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location)
File format - text file
Delimiter - “,”
Get monthly count of primary crime type, sorted by month in ascending and 
number of crimes per type in descending order
Store the result in HDFS path /user/<YOUR_USER_ID>/solutions/solution01/crimes_by_type_by_month
Output File Format: TEXT
Output Columns: Month in YYYYMM format, crime count, crime type
Output Delimiter: \t (tab delimited)
Output Compression: gzip

pyspark --master yarn

# read
raw = sc.textFile("/public/crime/csv")
header = raw.first()
rawnohead=raw.filter(lambda x : x<>header)

def dateformat(v):
	temp=v.split(" ")[0]
	mon=str(temp.split("/")[0])
	yr=str(temp.split("/")[2])
	return(yr+mon)

from pyspark.sql import Row

# format
ft=rawnohead.map(lambda x : x.split(","))\
			.map(lambda x : ((int(dateformat(x[2])),x[5]),1))\
			.reduceByKey(lambda x,y : x+y)\
			.map(lambda x : Row(x[0][0],x[1],x[0][1]))\
			.toDF(["yrmonth","crime count","crime type"])\
			.orderBy(["yrmonth","crime count"],ascending=[1,0])\
			.rdd.map(lambda x : str(x["yrmonth"])+("\t")+str(x["crime count"])+("\t")+x["crime type"])

# write
ft.coalesce(1).saveAsTextFile("/user/ramkarthik/ram/solution/crime/crime_by_type_by_month45",compressionCodecClass="org.apache.hadoop.io.compress.GzipCodec")