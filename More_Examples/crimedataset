Details - Duration 15 to 20 minutes
Data is available in HDFS file system under /public/crime/csv
Structure of data (ID,0
Case Number,1
Date,2
Block,3
IUCR,4
Primary Type,5
Description,6
Location Description,7
Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location)
File format - text file
Delimiter - “,” (use regex while splitting split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1), as there are some fields with comma and enclosed using double quotes.
Get top 3 crime types based on number of incidents in RESIDENCE area using “Location Description”
Store the result in HDFS path /user/<YOUR_USER_ID>/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA
Output Fields: Crime Type, Number of Incidents
Output File Format: JSON
Output Delimiter: N/A
Output Compression: No


pyspark --master yarn
from pyspark.sql import Row

raw = sc.textFile("/public/crime/csv")

hd = raw.first()
hdcol = [str(i).strip().replace(" ","") for i in hd.split(",")]
hdbr = sc.broadcast(hdcol)

def f(x, colname):
	d = {}
	for i in range(len(x)):
		d[colname[i]]=x[i]
	return d

ft = raw.filter(lambda x : x<>hd)\
		.map(lambda x : x.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1))\
		.map(lambda x : x[0].split(","))\
		.map(lambda x : Row(**f(x,hdbr.value)))\
		.toDF()\
		.select(hdbr.value)


for i,j in enumerate(hdcol) : print i, j


ft = raw.filter(lambda x : x<>hd)\
		.map(lambda x : x.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1))\
		.map(lambda x : x[0].split(","))\
		.map(lambda x : Row(Dt = str(x[2]),desc = x[7])) \
		.toDF()

ft.registerTempTable("tmp")

ftrs = sqlContext.sql("select cast(concat(substring(dt,7,4),substring(dt,1,2)) as bigint) as yrmn, \
desc, count(*) as cnt from tmp \
group by cast(concat(substring(dt,7,4),substring(dt,1,2)) as bigint), desc \
order by yrmn asc, cnt desc")


ftrs = sqlContext.sql("select yrmn as YEARMONTH, desc as LOCATION, cnt as Volume, \
dense_rank()over(partition by yrmn order by cnt desc) as rnk from \
( select cast(concat(substring(dt,7,4),substring(dt,1,2)) as bigint) as yrmn, \
desc, count(*) as cnt from tmp \
group by cast(concat(substring(dt,7,4),substring(dt,1,2)) as bigint), desc )inx \
having rnk <= 3")


ftrs.coalesce(1).write.mode("overwrite").format("json").save("/user/ramkarthik/ram/solution/crime/crime_by_location.json")


# testing
# sqlContext.read.json("/user/ramkarthik/ram/solution/crime/crime_by_location.json").show()


hd = raw.first()

for i,j in enumerate(hd.split(",")) : print(i,j)
0, u'ID')
(1, u'Case Number')
(2, u'Date')
(3, u'Block')
(4, u'IUCR')
(5, u'Primary Type')
(6, u'Description')
(7, u'Location Description')
(8, u'Arrest')
(9, u'Domestic')
(10, u'Beat')
(11, u'District')
(12, u'Ward')
(13, u'Community Area')
(14, u'FBI Code')
(15, u'X Coordinate')
(16, u'Y Coordinate')
(17, u'Year')
(18, u'Updated On')
(19, u'Latitude')
(20, u'Longitude')
(21, u'Location')





